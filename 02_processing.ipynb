{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from theonerig.core import *\n",
    "from theonerig.utils import *\n",
    "from theonerig.modelling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def eyetrack_stim_inten(stim_inten, eye_track, \n",
    "                        upsampling=2,\n",
    "                        eye_calib=[[94 ,18], [ 8, 59]],\n",
    "                        box_w=None, box_h=None):\n",
    "    \"\"\"From stimulus data and eye tracking, returns a corrected and upsampled stimulus data.\"\"\"\n",
    "    eye_x, eye_y = eye_track[:,0], eye_track[:,1]\n",
    "    shape_y, shape_x = stim_inten.shape[-2:]\n",
    "    if box_w is None:\n",
    "        box_w = 1280//shape_x\n",
    "    if box_h is None:\n",
    "        box_h = 720//shape_y\n",
    "\n",
    "    if shape_y>1 and shape_x>1:\n",
    "        box_w, box_h = int(box_w/upsampling), int(box_h/upsampling)\n",
    "    elif shape_y > 1:\n",
    "        box_w, box_h = box_w                , int(box_h/upsampling)\n",
    "    elif shape_x > 1:\n",
    "        box_w, box_h = int(box_w/upsampling), box_h\n",
    "    \n",
    "    eye_transfo_f = _eye_to_stim_f(eye_calib=eye_calib, \n",
    "                                  box_width=box_w,\n",
    "                                  box_height=box_h)\n",
    "    \n",
    "    if shape_y>1 and shape_x>1:\n",
    "        stim_inten = stim_inten.repeat(upsampling,axis=1).repeat(upsampling,axis=2)\n",
    "    elif shape_y > 1:\n",
    "        stim_inten = stim_inten.repeat(upsampling,axis=1)\n",
    "    elif shape_x > 1:\n",
    "        stim_inten = stim_inten.repeat(upsampling,axis=2)\n",
    "        \n",
    "    xpos_avg = np.mean(eye_x)\n",
    "    ypos_avg = np.mean(eye_y)\n",
    "    mean_stim_inten = int((np.max(stim_inten)+np.min(stim_inten))/2)\n",
    "    #After getting the shift of the matrix to apply, we roll the matrix instead of extending it to the shifts\n",
    "    #This seems strange, but from the cell point of view, that is potentially looking at no stimulus,\n",
    "    # the response it gives are uncorrelated with the stimulus, and so shouldn't impact further analysis\n",
    "    # Advantage is that it keeps the data small enough, without loosing regions of the stimulus.\n",
    "    for i in range(len(stim_inten)):\n",
    "        x_eyeShift = eye_x[i]-xpos_avg\n",
    "        y_eyeShift = eye_y[i]-ypos_avg\n",
    "        \n",
    "        stim_shift_x, stim_shift_y = eye_transfo_f(x_eyeShift=x_eyeShift,\n",
    "                                                   y_eyeShift=y_eyeShift) \n",
    "        if shape_y>1 and shape_x>1:\n",
    "            rolled_stim = np.roll(stim_inten[i],stim_shift_y,axis=0)\n",
    "            rolled_stim = np.roll(rolled_stim  ,stim_shift_x,axis=1)\n",
    "        elif shape_y > 1:\n",
    "            rolled_stim = np.roll(stim_inten[i],stim_shift_y,axis=0)\n",
    "        elif shape_x > 1:\n",
    "            rolled_stim = np.roll(stim_inten[i],stim_shift_x,axis=1)\n",
    "        stim_inten[i] = rolled_stim\n",
    "        \n",
    "    return stim_inten\n",
    "\n",
    "def _eye_to_stim_f(eye_calib, box_width, box_height):\n",
    "    eye_to_stim     = np.linalg.inv(eye_calib)\n",
    "    box_dim         = np.array([1280/(box_width), 720/(box_height)])\n",
    "    return partial(_linear_transform, box_dim=box_dim, transfo_matrix=eye_to_stim)\n",
    "    \n",
    "def _linear_transform(box_dim, transfo_matrix, x_eyeShift, y_eyeShift):\n",
    "    transform_coord = np.dot(transfo_matrix, np.array([x_eyeShift, y_eyeShift]).T)\n",
    "    stim_vec        = np.round(transform_coord * box_dim).astype(int)\n",
    "    return stim_vec[0], -stim_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Load all the test data\n",
    "from theonerig.testdata import *\n",
    "locals().update(load_vivo_2p())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Data_Pipe(reM, [\"checkerboard\", \"eye_tracking\", \"S_matrix\"], [\"stim_inten\", \"eye_track\", \"spike_counts\"])\n",
    "pipe += \"checkerboard\"\n",
    "reM.plot()\n",
    "pipe.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_inten_corrected = eyetrack_stim_inten(pipe[0][\"stim_inten\"], pipe[0][\"eye_track\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe[0][\"stim_inten\"].shape)\n",
    "print(stim_inten_corrected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process_sta_batch(stim_inten, spike_counts, Hw=30, Fw=2):\n",
    "    \"\"\"Calculate the STA for a batch of cells.\"\"\"\n",
    "    #Preparing the stimulus\n",
    "    shape_y, shape_x = stim_inten.shape[-2:]\n",
    "    stim_inten = stim_inten_norm(stim_inten)\n",
    "    \n",
    "    #We just have to calculate one STA over the whole record\n",
    "    stim_inten   = np.reshape(stim_inten, (len(stim_inten),-1))\n",
    "    stim_inten   = np.transpose(stim_inten)\n",
    "    allCells_sta = _staEst_fromBins(stim_inten, spike_counts, Hw, Fw=Fw)\n",
    "\n",
    "    for k, cell_sta in enumerate(allCells_sta): #Easy way to do normalization for each cell that works for all possible shapes\n",
    "        allCells_sta[k] = np.nan_to_num(cell_sta/np.max(np.abs(cell_sta)))\n",
    "    if shape_y>1 and shape_x>1:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw, shape_y, shape_x))\n",
    "    elif shape_y>1 or shape_x>1:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw,-1))\n",
    "    else:\n",
    "        allCells_sta = allCells_sta.reshape((len(allCells_sta),Hw+Fw))\n",
    "\n",
    "    return allCells_sta\n",
    "\n",
    "def _staEst_fromBins(stim, spike_counts, Hw, Fw=0):\n",
    "    \"\"\"Fw is the amount of frame after a spike that we calculate the average for.\n",
    "        stim must be of shape (x*y,n_frame)\n",
    "        spike_counts must be of shape (n_frame, n_cells)\n",
    "        Return sta of all cells in the shape (n_cells, Hw+Fw, y*x) \"\"\"\n",
    "    spike_counts[:Hw] = 0\n",
    "    \n",
    "    spike_counts = np.nan_to_num(spike_counts / np.sum(spike_counts,axis=0))\n",
    "    spike_counts = spike_counts - np.mean(spike_counts,axis=0)\n",
    "    sta = np.zeros((Hw+Fw, stim.shape[0], spike_counts.shape[-1]))\n",
    "    for i in range(Hw):\n",
    "        sta[(Hw-1-i),:,:] = np.dot(stim, spike_counts)\n",
    "        spike_counts = np.roll(spike_counts, -1, axis=0)\n",
    "    spike_counts = np.roll(spike_counts, Hw, axis=0)\n",
    "    if Fw != 0:\n",
    "        spike_counts[-Fw:] = 0\n",
    "    for i in range(Fw):\n",
    "        spike_counts  = np.roll(spike_counts, 1, axis=0)\n",
    "        sta[Hw+i,:,:] = np.dot(stim, spike_counts)\n",
    "    spike_counts = np.roll(spike_counts, -Fw, axis=0)\n",
    "    return np.transpose(sta, (2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sta = process_sta_batch(eyetrack_stim_inten(pipe[0][\"stim_inten\"], pipe[0][\"eye_track\"]), \n",
    "                               pipe[0][\"spike_counts\"], Hw=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape is (n_cell, Hw+Fw, y, x)\n",
    "result_sta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 0\n",
    "plt.figure()\n",
    "plt.imshow(result_sta[cell_id, -15], vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_correlation(spike_counts, tail_len=50):\n",
    "    \"\"\"From `spike_counts` of shape (n_dpoints, n_cell), calculate the cross correlation\n",
    "    of the cells over a window of `tail_len`*2 centered on the middle of the trace.\"\"\"\n",
    "    n_dpoints, n_cell = spike_counts.shape\n",
    "    corr_arr = np.zeros((n_cell,n_cell,tail_len*2+1))\n",
    "    spike_counts = (spike_counts / np.max(spike_counts, axis=0)) #Independant normalization of the cells\n",
    "    spike_counts_edged = np.concatenate((np.zeros((tail_len,n_cell)), \n",
    "                    spike_counts, \n",
    "                    np.zeros((tail_len,n_cell)))) #Creating an array with 0 tails on both side to use the valid mode\n",
    "                                                  # of numpy.correlate\n",
    "    for i in range(n_cell):\n",
    "        for j in range(i, n_cell):\n",
    "            corr_arr[i,j] = np.correlate(spike_counts_edged[:,i],\n",
    "                                         spike_counts[:,j], \n",
    "                                         mode=\"valid\")\n",
    "            corr_arr[j,i] = corr_arr[i,j]\n",
    "    return corr_arr/n_dpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Data_Pipe(reM, [\"S_matrix\"])\n",
    "pipe += \"checkerboard\"\n",
    "checker_corr = cross_correlation(pipe[0][\"S_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cell = pipe[0][\"S_matrix\"].shape[1]\n",
    "ymin, ymax = np.min(checker_corr), np.max(checker_corr)\n",
    "for i in range(n_cell):\n",
    "    for j in range(i, n_cell):\n",
    "        plt.subplot(n_cell,n_cell,1+i*n_cell+j)\n",
    "        plt.plot(checker_corr[i,j])\n",
    "        plt.ylim([ymin,ymax])\n",
    "        plt.subplot(n_cell,n_cell,1+j*n_cell+i)\n",
    "        plt.plot(checker_corr[i,j])\n",
    "        plt.ylim([ymin,ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def corrcoef(spike_counts):\n",
    "    return np.corrcoef(spike_counts.T)\n",
    "\n",
    "def flatten_corrcoef(corrcoef_matrix):\n",
    "    shp = corrcoef_matrix.shape\n",
    "    return np.array([corrcoef_matrix[i,j] for i in range(shp[0]) for j in range(i+1, shp[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Data_Pipe(reM, [\"S_matrix\"])\n",
    "pipe += \"checkerboard\"\n",
    "corr = corrcoef(pipe[0][\"S_matrix\"])\n",
    "print(corr)\n",
    "print(flatten_corrcoef(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stimulus_ensemble(stim_inten, Hw=30, x=0, y=0, w=None, h=None):\n",
    "    stim_inten = stim_inten_norm(stim_inten)\n",
    "    if w is None:\n",
    "        w = stim_inten.shape[2]\n",
    "    if h is None:\n",
    "        h = stim_inten.shape[1]\n",
    "    stim_ensmbl = np.zeros((len(stim_inten)-Hw, w*h*Hw))\n",
    "    for i in range(Hw, len(stim_inten)):\n",
    "        flat_stim         = np.ndarray.flatten(stim_inten[i-Hw:i,\n",
    "                                                          y:y+h,\n",
    "                                                          x:x+w])\n",
    "        stim_ensmbl[i-Hw] = flat_stim\n",
    "    return stim_ensmbl\n",
    "\n",
    "def process_nonlinearity(stim_ensemble, spike_bins):\n",
    "    \"\"\"Stimulus must already have been converted to the stim_ensemble, so spike_bins must also\n",
    "    not include the history window a the beggining.\"\"\"\n",
    "    assert len(stim_ensemble)==len(spike_bins)\n",
    "    stim_ensmbl_mean  = np.mean(stim_ensemble,axis=0)#np.mean(spike_triggering_stimuli,axis=0)\n",
    "    spike_ensmbl_mean = np.average(stim_ensemble, axis=0, weights=spike_bins)\n",
    "    middle_vec = np.mean(np.stack((stim_ensmbl_mean, spike_ensmbl_mean)), axis=0)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    fit = pca.fit(np.stack((stim_ensmbl_mean,\n",
    "                            spike_ensmbl_mean,\n",
    "                            middle_vec)))\n",
    "    stim_ensemble_tranfo = fit.transform(stim_ensemble)\n",
    "    \n",
    "    if np.min(spike_bins)<1:#We have probabilities, not spike counts. Need to make it integers\n",
    "        mask        = np.where(spike_bins > 0)[0]\n",
    "        nonzero_min = np.min(spike_bins[mask])\n",
    "        discretized = spike_bins/nonzero_min\n",
    "        spike_bins  = ((10*discretized)/(np.max(discretized))).astype(int)\n",
    "    spike_ensembl = []\n",
    "    for n_spike, stim_transfo in zip(spike_bins, stim_ensemble_tranfo):\n",
    "        spike_ensembl.extend([stim_transfo]*n_spike)\n",
    "    \n",
    "    xaxis      = np.linspace(np.min(stim_ensemble_tranfo),np.max(stim_ensemble_tranfo),101)\n",
    "    hist_all   = np.histogram(stim_ensemble_tranfo, bins=xaxis)[0]\n",
    "    hist_trigg = np.histogram(spike_ensembl, bins=xaxis)[0]\n",
    "    \n",
    "    nonlin = hist_trigg/hist_all\n",
    "    \n",
    "    nonlin = fill_nan(nonlin)\n",
    "        \n",
    "    return nonlin\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Data_Pipe(reM, [\"S_matrix\"], [\"spike_counts\"])\n",
    "pipe += \"checkerboard\"\n",
    "checker_corr = cross_correlation(pipe[0][\"spike_counts\"])\n",
    "stim_ensembl = stimulus_ensemble(stim_inten_corrected)\n",
    "nonlin = process_nonlinearity(stim_ensembl, pipe[0][\"spike_counts\"][30:,0])\n",
    "plt.figure()\n",
    "plt.plot(nonlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def activity_histogram(spike_counts):\n",
    "    flat_spikes = spike_counts.reshape(-1)\n",
    "    flat_cell = np.array([[i]*spike_counts.shape[0] for i in range(spike_counts.shape[1])]).reshape(-1)\n",
    "    hist = np.histogram2d(flat_spikes, flat_cell, bins=[100,spike_counts.shape[1]])[0] / spike_counts.shape[0]\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Data_Pipe(reM, [\"S_matrix\"], [\"spike_counts\"])\n",
    "pipe += \"darkness\"\n",
    "pipe &= \"S_matrix\"\n",
    "hist_dark = activity_histogram(pipe[0][\"spike_counts\"])\n",
    "\n",
    "pipe = Data_Pipe(reM, [\"S_matrix\"], [\"spike_counts\"])\n",
    "pipe += \"water\"\n",
    "hist_water = activity_histogram(pipe[0][\"spike_counts\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_dark[1:])\n",
    "plt.plot(hist_water[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_distances(masks):\n",
    "    \"\"\"Compute distances from the center of mass of a list of mask. masks must be in shape (n_mask, y, x)\"\"\"\n",
    "    center_mass = np.array([ndimage.measurements.center_of_mass(mask) for mask in masks])\n",
    "    cross_distances = np.zeros((len(masks),len(masks)))\n",
    "    for i in range(len(masks)):\n",
    "        for j in range(i,len(masks)):\n",
    "            cross_distances[i,j] = np.linalg.norm(center_mass[i]-center_mass[j])\n",
    "            cross_distances[j,i] = cross_distances[i,j]\n",
    "    return cross_distances\n",
    "\n",
    "def cross_distances_sta(fits, sta_shape, f):\n",
    "    sta_masks = np.array([img_2d_fit(sta_shape, fit, f) for fit in fits])\n",
    "    for i,sta_mask in enumerate(sta_masks):\n",
    "        if abs(np.min(sta_mask)) > np.max(sta_mask):\n",
    "            sta_masks[i] = sta_mask < -.5\n",
    "        else:\n",
    "            sta_masks[i] = sta_mask > .5\n",
    "    return cross_distances(sta_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_matrix = np.load(r\".\\files\\vivo_2p\\cells_spatial_matrix.npy\")\n",
    "cross_distances(A_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
