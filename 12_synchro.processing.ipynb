{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp synchro.processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_thresholds(data):\n",
    "    max_val = max(data[len(data)//2:len(data)//2 + 10000000]) #Looking for a max in a portion of the data, from the middle\n",
    "    high_thresh = max_val*3/4 # High threshold set at 3/4th of the max\n",
    "    low_thresh  = max_val*1/4\n",
    "    return low_thresh, high_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_first_high(data, threshold):\n",
    "    if np.any(data>threshold):\n",
    "        return np.argmax(data>threshold)\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def reverse_detection(data, frame_timepoints, low_threshold, increment):\n",
    "    new_timepoints = []\n",
    "    new_signals = []\n",
    "\n",
    "    safe_increment = int(increment * 105/100)\n",
    "\n",
    "    i = frame_timepoints[0]-safe_increment\n",
    "    while i>0:\n",
    "        data_slice = data[i:i+increment//2]\n",
    "        if np.any(data_slice > low_threshold):\n",
    "            i = i+np.argmax(data_slice > low_threshold)\n",
    "        else:\n",
    "            break #No low threshold crossing found -> no more frames to detect\n",
    "        new_timepoints.append(i)\n",
    "        i-= safe_increment #We move backward of almost a frame\n",
    "\n",
    "    return new_timepoints[::-1]\n",
    "    \n",
    "def extend_timepoints(frame_timepoints, n=10):\n",
    "    frame_timepoints = np.array(frame_timepoints)\n",
    "    typical_distance = int(np.mean(np.diff(frame_timepoints)))\n",
    "    extended_tp = [frame_timepoints[0]-(i+1)*typical_distance for i in range(n) if (frame_timepoints[0]-(i+1)*typical_distance)>0]\n",
    "    return extended_tp[::-1]\n",
    "    \n",
    "def detect_frames(data, low_threshold, high_threshold, increment):\n",
    "    frame_timepoints, frame_signals = [], []\n",
    "    safe_increment = int(increment*95/100)\n",
    "\n",
    "    first_high = get_first_high(data, high_threshold)\n",
    "    if first_high == -1:\n",
    "        print(\"No high frame detected. Detection can't work.\")\n",
    "        return\n",
    "\n",
    "    frame_timepoints.append(first_high)\n",
    "    frame_signals.append(1)\n",
    "\n",
    "    new_timepoints   = reverse_detection(data, frame_timepoints, low_threshold, increment)\n",
    "    new_extrapolated = extend_timepoints(new_timepoints)\n",
    "    frame_timepoints = new_extrapolated + new_timepoints + frame_timepoints\n",
    "    frame_signals    = [0]*(len(new_timepoints)+len(new_extrapolated)) + frame_signals\n",
    "\n",
    "    i = first_high + safe_increment\n",
    "    while i < len(data):\n",
    "        data_slice = data[i:i+increment//2]\n",
    "        if np.any(data_slice>low_threshold):\n",
    "            i = i+np.argmax(data_slice>low_threshold)\n",
    "        else:\n",
    "            break #This frame sequence is over. Pass the next sequence through this function if there are frames left\n",
    "        frame_timepoints.append(i)\n",
    "        frame_signals.append(int(np.any(data_slice > high_threshold)))\n",
    "        i += safe_increment\n",
    "\n",
    "    frame_timepoints = np.array(frame_timepoints)\n",
    "    frame_signals    = np.array(frame_signals)\n",
    "    frame_timepoints = frame_timepoints - 3 # A slight shift of the timepoints \n",
    "                                            # to include the begginning of the peaks.\n",
    "        \n",
    "    error_check(frame_timepoints)\n",
    "\n",
    "#     self.frame_start_time = self.record_start_time + datetime.timedelta(0,int(self.frame_timepoints[0]/self.sampling_rate))\n",
    "#     self.frame_end_time = self.record_start_time + datetime.timedelta(0,int(self.frame_timepoints[-1]/self.sampling_rate))\n",
    "\n",
    "    return frame_timepoints, frame_signals\n",
    "\n",
    "def error_check(frame_tp):\n",
    "    deriv_frame_tp = np.diff(frame_tp)\n",
    "    error_len_th = np.mean(deriv_frame_tp)+np.std(deriv_frame_tp)*6\n",
    "\n",
    "    error_frames = np.abs(deriv_frame_tp)>error_len_th\n",
    "    if np.any(error_frames):\n",
    "        print(\"Error in timepoints detected in frames\", np.where(error_frames)[0], \n",
    "              \"at timepoint\", frame_tp[np.where(error_frames)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cluster_frame_signals(data, frame_timepoints, n_cluster=5):\n",
    "    frame_aucs = np.fromiter(map(np.trapz, np.split(data, frame_timepoints)), float)\n",
    "    if frame_timepoints[0] != 0: #We need to remove the first part if it wasn't a full frame\n",
    "        frame_aucs = frame_aucs[1:]\n",
    "    frame_auc_sorted = np.sort(frame_aucs)\n",
    "    deriv = np.array(frame_auc_sorted[1:]-frame_auc_sorted[:-1])\n",
    "    deriv[:5]  = 0 #removing tails values that can show weird stuff\n",
    "    deriv[-5:] = 0\n",
    "    threshold_peak = np.std(deriv)*3\n",
    "    n          = n_cluster - 1\n",
    "    idx_gaps = np.zeros(n+3, dtype=\"int\")\n",
    "    tmp_deriv = deriv.copy()\n",
    "    for i in range(n+3): #Detecting more peaks than needed and then taking them starting on the right\n",
    "        if tmp_deriv[np.argmax(tmp_deriv)] < threshold_peak:\n",
    "            if i<n_cluster-1:\n",
    "                print(\"Less transition in AUC detected than needed, results will be weird\")\n",
    "            break\n",
    "        idx_gaps[i] = np.argmax(tmp_deriv)\n",
    "        tmp_deriv[idx_gaps[i]] = 0\n",
    "    idx_gaps = np.sort(idx_gaps)\n",
    "    idx_gaps = idx_gaps[-4:]\n",
    "    thresholds = np.zeros(n, dtype=\"float\")\n",
    "    for i, idx in enumerate(idx_gaps):\n",
    "        thresholds[i] = (frame_auc_sorted[idx+1] + frame_auc_sorted[idx])/2\n",
    "\n",
    "    return np.array([np.sum(auc>thresholds) for auc in frame_aucs], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_time(time_str, pattern=\"%y%m%d_%H%M%S\"):\n",
    "    return datetime.datetime.strptime(time_str, pattern)\n",
    "\n",
    "def get_position_estimate(stim_time, record_time, sampling_rate):\n",
    "    if stim_time < record_time:\n",
    "        return -1\n",
    "    else:\n",
    "        return (stim_time - record_time).seconds * sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_starting_position(frame_timepoints, frame_signals, stim_signals, estimate_start):\n",
    "    stim_matching_len = min(600, np.where(np.diff(stim_signals)!=0)[0][50]) #Way of getting the 50th change in the signals\n",
    "    #But not higher than 600 (correspond to 10s, and is necessary for moving gratings)\n",
    "#     stim_matching_len = 50\n",
    "    idx_estimate = np.argmax(frame_timepoints>estimate_start)\n",
    "    search_slice = slice(max(0, idx_estimate-1000), min(idx_estimate+1000, len(frame_signals)))\n",
    "#     diff_signals = np.diff(frame_signals[search_slice])\n",
    "#     diff_stim    = np.diff(stim_signals[:stim_matching_len])\n",
    "#     return search_slice.start + np.argmax(np.correlate(diff_signals, diff_stim))\n",
    "    return search_slice.start + np.argmax(np.correlate(frame_signals[search_slice], \n",
    "                                                       stim_signals[:stim_matching_len]))\n",
    "\n",
    "def display_match(match_position, reference=None, recorded=None, corrected=None, len_line=50):\n",
    "    start, mid, end = 0, len(reference)//2, len(reference)-len_line\n",
    "    for line in [start, mid, end]:\n",
    "        if reference is not None:\n",
    "            print(\"REF [\"+str(line)+\"] \",\" \".join(map(str,map(int, reference[line:line+len_line]))))\n",
    "        if recorded is not None:\n",
    "            print(\"REC [\"+str(line)+\"] \",\" \".join(map(str,map(int, recorded[line+match_position:line+len_line+match_position]))))\n",
    "        if corrected is not None:\n",
    "            print(\"COR [\"+str(line)+\"] \",\" \".join(map(str,map(int, corrected[line:line+len_line]))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def frame_error_correction(signals, unpacked, algo=\"nw\"):\n",
    "    if algo==\"nw\":\n",
    "        shift_log = shift_detection_NW(signals.astype(int), unpacked[1].astype(int))\n",
    "    elif algo==\"conv\":\n",
    "        shift_log = shift_detection_conv(signals.astype(int), unpacked[1].astype(int), range_=5)\n",
    "    intensity, marker, shader = apply_shifts(unpacked, shift_log)\n",
    "    error_frames, replacements = error_frame_matches(signals, marker, range_=5)\n",
    "    intensity[error_frames]    = intensity[replacements]\n",
    "    marker[error_frames]       = marker[replacements]\n",
    "    if shader is not None:\n",
    "        shader[error_frames] = shader[replacements]\n",
    "    return (intensity, marker, shader), shift_log, list(zip(map(int,error_frames), map(int,replacements)))\n",
    "\n",
    "def error_frame_matches(signals, marker, range_):\n",
    "    error_frames = np.nonzero(signals!=marker)[0]\n",
    "    where_equal = [((np.where(marker[err_id-range_:err_id+(range_+1)] == signals[err_id])[0]) - range_) for err_id in error_frames]\n",
    "    \n",
    "    #filtering out the frames where no match was found\n",
    "    tmp    = np.array([[wheq,err] for (wheq, err) in zip(where_equal, error_frames) if len(wheq)>0])\n",
    "    where_equal  = tmp[:,0]\n",
    "    error_frames = tmp[:,1]\n",
    "\n",
    "    #Choosing among the equal frame signals the one that is the closest\n",
    "    closest_equal = [wheq[(np.abs(wheq)).argmin()] for wheq in where_equal]\n",
    "    \n",
    "    error_frames = np.array(error_frames, dtype=int)\n",
    "    replacements  = error_frames + np.array(closest_equal, dtype=int)\n",
    "\n",
    "    return error_frames, replacements\n",
    "\n",
    "def apply_shifts(unpacked, op_log):\n",
    "    intensity, marker, shader = unpacked[0], unpacked[1], None\n",
    "    if len(unpacked)==3:\n",
    "        shader = unpacked[2]\n",
    "        \n",
    "    res_inten, res_marker = np.zeros(intensity.shape), np.zeros(marker.shape, dtype=int)\n",
    "    res_shader=None\n",
    "    if shader is not None:\n",
    "        res_shader = np.zeros(shader.shape)\n",
    "    cursor, shift = 0,0\n",
    "    for idx, op in op_log:\n",
    "        res_inten[cursor+shift:idx] = intensity[cursor:idx-shift]\n",
    "        res_marker[cursor+shift:idx] = marker[cursor:idx-shift]\n",
    "        if shader is not None:\n",
    "            res_shader[cursor+shift:idx] = shader[cursor:idx-shift]\n",
    "        if op==\"ins\": #We add the duplicated frame\n",
    "            res_inten[idx]  = intensity[idx-1] #duplicating\n",
    "            res_marker[idx] = marker[idx-1]\n",
    "            if shader is not None:\n",
    "                res_shader[idx] = shader[idx-1]\n",
    "            shift += 1 #And incrementing index\n",
    "        elif op==\"del\": \n",
    "            shift -= 1\n",
    "        cursor += len(marker[cursor:idx-shift])\n",
    "        \n",
    "    res_inten[cursor+shift:] = intensity[cursor:cursor+len(res_inten[cursor+shift:])]\n",
    "    res_marker[cursor+shift:] = marker[cursor:cursor+len(res_marker[cursor+shift:])]\n",
    "    if shader is not None:\n",
    "        res_shader[cursor+shift:] = shader[cursor:cursor+len(res_shader[cursor+shift:])]\n",
    "        \n",
    "    return (res_inten, res_marker, res_shader)\n",
    "\n",
    "def shift_detection_conv(signals, marker, range_):\n",
    "    marker = marker.copy()\n",
    "    shift_detected = True\n",
    "    operation_log = []\n",
    "    while shift_detected:\n",
    "        error_frames, replacements = error_frame_matches(signals, marker, range_)\n",
    "\n",
    "        all_shifts = np.zeros(len(marker))\n",
    "        all_shifts[error_frames] = replacements-error_frames\n",
    "        all_shifts_conv = np.convolve(all_shifts, [1/20]*20, mode=\"same\") #Averaging the shifts to find consistant shifts\n",
    "\n",
    "        shift_detected = np.any(np.abs(all_shifts_conv)>.5)\n",
    "        if shift_detected: #iF the -.5 threshold is crossed, we insert a \"fake\" frame in the reference and we repeat the operation\n",
    "            change_idx = np.argmax(np.abs(all_shifts_conv)>.5)\n",
    "            if all_shifts_conv[change_idx]>.5:#Need to delete frame in reference\n",
    "                #Need to refine index to make sure we delete a useless frame\n",
    "                start,stop = max(0,change_idx-2), min(len(marker),change_idx+2)\n",
    "                for i in range(start,stop):\n",
    "                    if marker[i] not in signals[start:stop]:\n",
    "                        change_idx = i\n",
    "                        break\n",
    "                operation_log.append([int(change_idx), \"del\"])\n",
    "                marker = np.concatenate((marker[:change_idx], marker[change_idx+1:], [0]))\n",
    "            else:#Need to insert frame in reference\n",
    "                operation_log.append([int(change_idx), \"ins\"])\n",
    "                #inserting a frame and excluding the last frame to keep the references the same length\n",
    "                marker     = np.insert(marker, change_idx, marker[change_idx], axis=0)[:-1] \n",
    "    return operation_log\n",
    "\n",
    "def shift_detection_NW(signals, marker):\n",
    "    \"\"\"Memory optimized Needleman-Wunsch algorithm.\n",
    "    Instead of an N*N matrix, it uses a N*(side*2+1) matrix. Indexing goes slightly differently but\n",
    "    result is the same, with far less memory consumption and exection speed scaling better with\n",
    "    size of the sequences to align.\"\"\"\n",
    "    #Setting the similarity matrix\n",
    "    side = 5\n",
    "    sim_mat = np.empty((len(marker), side*2+1), dtype=\"int32\")\n",
    "    #Setting the errors\n",
    "    insertion_v = -10 #insertions are commons not so high penalty\n",
    "    deletion_v  = -10 #deletions detection happens during periods of confusion but are temporary. High value\n",
    "    error_match = np.array([1,-1,-3,-3,-1]) #The value for a 0 matching with [0,1,2,3,4]\n",
    "    error_mat = np.empty((5,5))\n",
    "    for i in range(5):\n",
    "        error_mat[i] = np.roll(error_match,i)\n",
    "                \n",
    "    #Filling the similarity matrix\n",
    "    sim_mat[0, side] = error_mat[marker[0], signals[0]]\n",
    "    #Initialization: Setting the score of the first few row and first few column cells\n",
    "    for j in range(side+1, side*2+1):\n",
    "        sim_mat[0,j] = sim_mat[0,side] + insertion_v*j\n",
    "    for i in range(1, side+1):\n",
    "        sim_mat[i,side-i] = sim_mat[0,side] + deletion_v*i\n",
    "          \n",
    "    #Corpus: if j is the first cell of the row, the insert score is set super low\n",
    "    #        if j is the last  cell of the row, the delete score is set super low\n",
    "    for i in range(1, sim_mat.shape[0]):\n",
    "        start = max(side-i+1, 0)\n",
    "        stop  = min(side*2+1, side+sim_mat.shape[0]-i)\n",
    "        for j in range(start, stop):\n",
    "            if j==0:#j==start and i>side:\n",
    "                insert = -99999\n",
    "                delete = sim_mat[i-1, j+1] + deletion_v\n",
    "            elif j==side*2:\n",
    "                delete = -99999\n",
    "                insert = sim_mat[i, j-1] + insertion_v\n",
    "            else:\n",
    "                insert = sim_mat[i, j-1] + insertion_v\n",
    "                delete = sim_mat[i-1, j+1] + deletion_v\n",
    "            match  = sim_mat[i-1, j] + error_mat[marker[i], signals[j+i-side]]\n",
    "            sim_mat[i,j] = max(insert,delete,match)\n",
    "            \n",
    "    #Reading the similarity matrix\n",
    "    #In general, it's the same, at the difference that when i decrement, must add 1 to j compared to usual.\n",
    "    i = len(marker)-1\n",
    "    j = side\n",
    "    operation_log = []\n",
    "    while (i > 0 or j>side-i):\n",
    "        if (i > 0 and j>side-i and sim_mat[i,j]==(sim_mat[i-1,j]+error_mat[marker[i], signals[j+i-side]])):\n",
    "            i -= 1\n",
    "        elif(i > 0 and sim_mat[i,j] == sim_mat[i-1,j+1] + deletion_v):\n",
    "            operation_log.insert(0,(i+1, \"del\")) #Insert at i+1 (and j+1 bello) showed better empirical results\n",
    "            i-=1\n",
    "            j+=1\n",
    "        else:\n",
    "            operation_log.insert(0,(j+i-side+1, \"ins\"))\n",
    "            j-=1\n",
    "                \n",
    "    return operation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
