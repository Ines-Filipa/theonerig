{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.test import test_eq\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Useful functiosn to reshape/arrange/reduce raw data into clean data to add to the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Sequence, Union, Callable\n",
    "import scipy.interpolate as interpolate\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "from theonerig.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extend_sync_timepoints(timepoints:np.ndarray, signals:np.ndarray, \n",
    "                           up_bound, low_bound=0) -> Tuple[DataChunk, DataChunk]:\n",
    "    \"\"\"From `timepoints` and `signals` list, extend it on the left so it includes `low_bound`, and extend it\n",
    "    up to `up_bound`.\n",
    "    Return the new timepoints and signals as DataChunk objets with the number of timepoints added to the left\n",
    "    as a tuple.\n",
    "    \"\"\"\n",
    "    assert len(timepoints) == len(signals)\n",
    "    timepoints = np.array(timepoints)\n",
    "    signals = np.array(signals)\n",
    "    spb = np.mean(timepoints[1:]-timepoints[:-1]) #spf: sample_per_bin\n",
    "        \n",
    "    #Left and right side are just prolongation of the sample_times up \n",
    "    # from (0-sample_per_fr) to (len+sample_per_fr) so it covers all timepoints\n",
    "    left_side  = np.arange(timepoints[0]-spb , low_bound - spb, -spb)[::-1].astype(int)\n",
    "    right_side = np.arange(timepoints[-1]+spb,  up_bound + spb,  spb).astype(int)\n",
    "\n",
    "    new_timepoints = np.concatenate((left_side, \n",
    "                                     timepoints, \n",
    "                                     right_side))\n",
    "    \n",
    "    timepoint_chunk = DataChunk(data=new_timepoints, idx=0, group=\"sync\")\n",
    "    signal_chunk    = DataChunk(data=signals, idx=len(left_side), group=\"sync\")\n",
    "    return (timepoint_chunk, signal_chunk, len(left_side))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It extends the timepoints by finding the typical distance between timepoints so it includes both left and right limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_tp, extended_sig, n_left = extend_sync_timepoints([7900,8900,9900], signals=[1,2,3], up_bound=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extended_tp, end=\"\\n\\n\")\n",
    "print(extended_sig, end=\"\\n\\n\")\n",
    "print(\"N timepoints added to the left:\",n_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def align_sync_timepoints(timepoints:np.ndarray, signals:np.ndarray,\n",
    "                          ref_timepoints:DataChunk, ref_signals:DataChunk, \n",
    "                          shift=None) -> DataChunk:\n",
    "    \"\"\"Align the `signals` of a `timepoints` timeserie to a reference `ref_timepoints` with the corresponding\n",
    "    `ref_signals`. A `shift` can be directly specified, otherwise it will be searched by finding the maximum\n",
    "    of the correlations of the two signals timeseries.\n",
    "    Returns a DataChunk of the aligned timepoints\"\"\"\n",
    "    assert len(timepoints) == len(signals)\n",
    "    timepoints = np.array(timepoints)\n",
    "    signals = np.array(signals)\n",
    "    \n",
    "    if shift is None: #If a shift is provided we use it, otherwise we use the max correlation\n",
    "        shift = np.argmax(np.correlate(ref_signals, signals, mode=\"valid\"))\n",
    "        \n",
    "    spb = np.mean(timepoints[1:]-timepoints[:-1]) #spf: sample_per_bin\n",
    "    n_left  = ref_signals.idx + shift\n",
    "    n_right = (len(ref_timepoints) \n",
    "               - len(timepoints) \n",
    "               - n_left)\n",
    "\n",
    "    init_left  = timepoints[0]-spb\n",
    "    init_right = timepoints[-1]+spb\n",
    "\n",
    "    left_side  = np.arange(init_left , init_left-(spb*n_left+1), -spb)[:n_left][::-1].astype(int)\n",
    "    right_side = np.arange(init_right, init_right+(spb*n_right+1), spb)[:n_right].astype(int)\n",
    "\n",
    "    new_timepoints = np.concatenate((left_side, \n",
    "                                     timepoints, \n",
    "                                     right_side))\n",
    "    return DataChunk(data=new_timepoints, idx=0, group=\"sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aligning these signals 1,2,3 to the previously extended timepoints and signals\n",
    "aligned_timepoints = align_sync_timepoints(timepoints=[1000,2000,3000], signals=[1,2,3],\n",
    "                     ref_timepoints=extended_tp, ref_signals=extended_sig)\n",
    "\n",
    "#We can observe 8 timepoints added to our function to match all existing frame of the reference.\n",
    "print(aligned_timepoints)\n",
    "test_eq(len(aligned_timepoints), len(extended_tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resample_to_timepoints(timepoints:np.ndarray, data:np.ndarray, \n",
    "                             ref_timepoints:DataChunk, group=\"data\") -> DataChunk:\n",
    "    \"\"\"Resample the `data` at the `timepoints` to an array at the timepoints of `ref_timepoints`.\n",
    "    Return a DataChunck of the resampled data belonging to `group`.\"\"\"\n",
    "    \n",
    "    assert len(timepoints) == len(data)\n",
    "    timepoints = np.array(timepoints)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    start_idx = np.argmax(timepoints[0] <=ref_timepoints)\n",
    "    stop_idx  = np.argmax(timepoints[-1]<=ref_timepoints)\n",
    "    \n",
    "    if len(ref_timepoints[start_idx:stop_idx]) < len(timepoints): #Downsampling\n",
    "        distance = (np.argmax(timepoints>ref_timepoints[start_idx+1]) \n",
    "                - np.argmax(timepoints>ref_timepoints[start_idx]))\n",
    "    \n",
    "        kernel = np.ones(distance)/distance\n",
    "        data = convolve1d(data, kernel, axis=0) #Smooting to avoid weird sampling\n",
    "\n",
    "    new_data = interpolate.interp1d(timepoints, data, axis=0)(ref_timepoints[start_idx:stop_idx])\n",
    "\n",
    "    idx = ref_timepoints.idx + start_idx\n",
    "    return DataChunk(data=new_data, idx = idx, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Downsampling\n",
    "np.random.seed(3)\n",
    "len_data = 40\n",
    "wrong_dim_data = np.random.rand(len_data)\n",
    "wrong_dim_timepoints = np.linspace(0,10000,len_data,endpoint=False, dtype=int)\n",
    "\n",
    "downsamp_data = resample_to_timepoints(wrong_dim_timepoints, wrong_dim_data, \n",
    "                                         ref_timepoints=extended_tp)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(wrong_dim_timepoints, wrong_dim_data)\n",
    "start,stop = downsamp_data.idx, downsamp_data.idx+len(downsamp_data)\n",
    "plt.plot(extended_tp[start:stop], downsamp_data)\n",
    "print(\"Orig len:\", len(wrong_dim_data), \"Target len:\",len(extended_tp[start:stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsampling\n",
    "np.random.seed(3)\n",
    "len_data = 5\n",
    "wrong_dim_data = np.random.rand(len_data)\n",
    "wrong_dim_timepoints = np.linspace(0,10000,len_data,endpoint=False, dtype=int)\n",
    "\n",
    "downsamp_data = resample_to_timepoints(wrong_dim_timepoints, wrong_dim_data, \n",
    "                                         ref_timepoints=extended_tp)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(wrong_dim_timepoints, wrong_dim_data)\n",
    "start,stop = downsamp_data.idx, downsamp_data.idx+len(downsamp_data)\n",
    "plt.plot(extended_tp[start:stop], downsamp_data)\n",
    "print(\"Orig len:\", len(wrong_dim_data), \"Target len:\",len(extended_tp[start:stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stim_to_dataChunk(stim_values, stim_start_idx, reference:DataChunk) -> DataChunk:\n",
    "    \"\"\"Factory function for DataChunk of a stimulus\"\"\"\n",
    "    return DataChunk(data=stim_values, idx = (reference.idx + stim_start_idx), group=\"stim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def spike_to_dataChunk(spike_timepoints, ref_timepoints:DataChunk) -> DataChunk:\n",
    "    \"\"\"`spike_timepoints` must be a dictionnary of cell spike_timepoints list. This function then\n",
    "    bins the \"\"\"\n",
    "    cell_keys = list(map(str, \n",
    "                         sorted(map(int, \n",
    "                                    spike_timepoints.keys()))))\n",
    "    cell_map = dict([ (cell_key, i) for i, cell_key in enumerate(cell_keys) ])\n",
    "    spike_bins = np.zeros((ref_timepoints.shape[0], len(cell_keys)))\n",
    "    bins = np.concatenate((ref_timepoints[:], [(ref_timepoints[-1]*2)-ref_timepoints[-2]]))\n",
    "    for i, cell in enumerate(cell_keys):\n",
    "        \n",
    "        spike_bins[:, i] = np.histogram(spike_timepoints[cell], bins)[0]\n",
    "        \n",
    "    datachunk = DataChunk(data=spike_bins, idx = ref_timepoints.idx, group=\"cell\")\n",
    "    datachunk.attrs[\"cell_map\"] = cell_map\n",
    "    return datachunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_stim_args(stim_name, stim_ref):\n",
    "    \"\"\"Function really specific to Asari Lab stimuli. Stimuli were stored as h5 files. This function parse\n",
    "    the attributes of the stimuli that were stored in the h5 references of the stimuli.\"\"\"\n",
    "    args = {}\n",
    "    if stim_name in [\"chirp_am\", \"chirp_fm\", \"chirp_co\"]:\n",
    "        #+ add on off timings at the beginning?\n",
    "        args[\"n_repeat\"] = int(stim_ref.attrs[\"n_repeat\"])\n",
    "    if stim_name in [\"chirp_fm\"]:\n",
    "        args[\"max_freq\"] = int(stim_ref.attrs[\"max_frequency\"])\n",
    "    if stim_name in [\"moving_gratings\"]:\n",
    "        #+ Convert to degree units\n",
    "        args[\"n_fr_stim\"] = int(stim_ref.attrs[\"n_frame_on\"])#.keys()\n",
    "        args[\"n_fr_interstim\"] = int(stim_ref.attrs[\"n_frame_off\"])\n",
    "        args[\"n_repeat\"] = int(stim_ref.attrs[\"n_repeat\"])\n",
    "        args[\"n_angle\"] = int(stim_ref.attrs[\"n_angle\"])\n",
    "        args[\"sp_freqs\"] = list(map(int,stim_ref.attrs[\"spatial_frequencies\"][1:-1].split(\",\")))\n",
    "        args[\"speeds\"] = list(map(int,stim_ref.attrs[\"speeds\"][1:-1].split(\",\")))\n",
    "    if stim_name in [\"flickering_bars\", \"checkerboard\", \"flickering_bars_pr\"]:\n",
    "        #Get the size of the sides in angle\n",
    "        pass\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def peak_sta_frame(sta):\n",
    "    abs_sta   = np.abs(sta)\n",
    "    idx_frame = np.unravel_index(abs_sta.argmax(), sta.shape)[0]\n",
    "    return sta[idx_frame]\n",
    "\n",
    "def stim_inten_norm(stim_inten):\n",
    "    stim_inten = stim_inten.astype(float)\n",
    "    stim_inten -= np.min(stim_inten)\n",
    "    stim_inten -= np.max(stim_inten)/2\n",
    "    stim_inten /= np.max(np.abs(stim_inten))\n",
    "    return np.round(stim_inten, 2)\n",
    "#     stim_inten[stim_inten==255] = 1\n",
    "#     stim_inten[stim_inten==0]   = -1\n",
    "#     stim_inten[(stim_inten==127) | (stim_inten==128)] = 0 #In case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def twoP_dataChunks(ref_timepoints:DataChunk, frame_timepoints, len_epochs, C_matrix, S_matrix):\n",
    "    C_datachunk_l = []\n",
    "    S_datachunk_l = []\n",
    "    cursor = 0\n",
    "    for i, len_epoch in enumerate(len_epochs):\n",
    "        start_idx = np.argmax(ref_timepoints>frame_timepoints[i][0])\n",
    "        stop_idx  = np.argmax(ref_timepoints>frame_timepoints[i][-6])\n",
    "        sub_C, sub_S = C_matrix.T[cursor:cursor+len_epoch], S_matrix.T[cursor:cursor+len_epoch]\n",
    "        cursor += len_epoch\n",
    "        f = interpolate.interp1d(range(len_epoch), sub_C, axis=0)\n",
    "        C_datachunk_l.append(DataChunk(data=f(np.linspace(0,len_epoch-1,stop_idx-start_idx)), \n",
    "                                       idx=start_idx, \n",
    "                                       group=\"cell\"))\n",
    "        f = interpolate.interp1d(range(len_epoch), sub_S, axis=0)\n",
    "        S_datachunk_l.append(DataChunk(data=f(np.linspace(0,len_epoch-1,stop_idx-start_idx)), \n",
    "                                       idx=start_idx, \n",
    "                                       group=\"cell\"))\n",
    "    return (C_datachunk_l, S_datachunk_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def img_2d_fit(shape, param_d, f):\n",
    "    y_, x_ = shape\n",
    "    xy = np.meshgrid(range(x_), range(y_))\n",
    "    return f(xy, **param_d).reshape(y_,x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fill_nan(A):\n",
    "    '''\n",
    "    interpolate to fill nan values. BRYAN WOODS@StackOverflow\n",
    "    '''\n",
    "    inds = np.arange(A.shape[0])\n",
    "    good = np.where(np.isfinite(A))\n",
    "    f = interpolate.interp1d(inds[good], A[good],bounds_error=False)\n",
    "    B = np.where(np.isfinite(A),A,f(inds))\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
