{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from collections import namedtuple \n",
    "from typing import Dict, Tuple, Sequence, Union\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "class DataChunk(np.ndarray):\n",
    "    \"\"\"Base brick of data.\"\"\"\n",
    "    def __new__(cls, data, idx, group, fill=0):\n",
    "        # See https://docs.scipy.org/doc/numpy-1.11.0/user/basics.subclassing.html#basics-subclassing\n",
    "        # for explanation on subclassing numpy arrays\n",
    "        obj = np.asarray(data).view(cls)\n",
    "        obj.idx = idx\n",
    "        obj.group = group\n",
    "        obj.fill = fill\n",
    "        \n",
    "        obj.attrs = {}\n",
    "        obj.slice = obj._get_slice()\n",
    "        obj.range = obj._range()\n",
    "        \n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None: return\n",
    "        self.idx = getattr(obj, 'idx', None)\n",
    "        self.group = getattr(obj, 'group', None)\n",
    "        self.fill = getattr(obj, 'fill', 0)\n",
    "        \n",
    "    def _range(self):\n",
    "        return range(self.idx, self.idx + len(self))\n",
    "    \n",
    "    def _get_slice(self):\n",
    "        return slice(self.idx, self.idx + len(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"Group: \"+str(self.group)\n",
    "                +\"\\nStarting index: \"+str(self.idx)\n",
    "                +\"\\nFilling value: \"+str(self.fill)\n",
    "                +\"\\n\"+super().__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"DataChunk(%s,%s,%s,%s)\"%(self.shape, self.idx, self.group, self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ContiguousRecord():\n",
    "    \"\"\"Representation of a contiguous recording session to store DataChunk\n",
    "    of various sources under a single time reference. DataChunk are stored\n",
    "    under a name in one of the groups \"sync\",\"stim\",\"data\" and \"cell\". \n",
    "    \n",
    "    A name can contain multiple DataChunk if those are not overlapping in time.\n",
    "    \n",
    "    Each ContiguousRecord contains in the group \"sync\" two master DataChunk,\n",
    "    one for signals to be recorded across acquisition device to syncronize them,\n",
    "    one for timepoints of these signals for the main device and are called \n",
    "    respectively \"signals\" and \"main_tp\".\n",
    "    \n",
    "    \"\"\"\n",
    "    MAIN_TP = \"main_tp\"\n",
    "    SIGNALS = \"signals\"\n",
    "    def __init__(self, length:int, signals:DataChunk, main_tp:DataChunk):\n",
    "        \"\"\"Instanciate a ContiguousRecord.\n",
    "        \n",
    "        Parameters:\n",
    "            length (int): Number of bins of this record\n",
    "            signals (DataChunk): Signals for this record\n",
    "            main_tp (DataChunk): Timepoints of the signals for the main device\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self._data_dict = {}  \n",
    "        \n",
    "        self[self.SIGNALS] = signals\n",
    "        self[self.MAIN_TP] = main_tp\n",
    "      \n",
    "    def dataset_intersect(self, existing_datachunk:list, new_datachunk:DataChunk):\n",
    "        range_new = set(new_datachunk.range)\n",
    "        intersect = False\n",
    "        for range_existing in existing_datachunk:\n",
    "            intersect |= len(range_new.intersection(range_existing.range)) > 0\n",
    "        return intersect\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._data_dict.keys()\n",
    "    \n",
    "    def get_slice(self, datachunk_name:str) -> list:\n",
    "        if datachunk_name in self._data_dict.keys():\n",
    "            return [chunk.slice for chunk in self._data_dict[datachunk_name]]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_names_group(self, group_name:str) -> list:\n",
    "        names = []\n",
    "        for key, dChunk_l in self._data_dict.items():\n",
    "            if dChunk_l[0].group == group_name:\n",
    "                names.append(key)\n",
    "        return names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __setitem__(self, key, value:DataChunk):\n",
    "        if isinstance(key, str):\n",
    "            if key not in self._data_dict.keys():\n",
    "                self._data_dict[key] = []\n",
    "                \n",
    "            if not self.dataset_intersect(self._data_dict[key], value):\n",
    "                self._data_dict[key].append(value)\n",
    "            else:\n",
    "                raise ValueError(\"Data with the same name already exists and intersect with the one provided\")\n",
    "        else:\n",
    "            raise KeyError(\"Cannot set data with an integer index, it needs a name\")\n",
    "                \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            l_datachunk = self._data_dict[key]\n",
    "            shape     = l_datachunk[0].shape\n",
    "            full_sequence = np.full(shape=(self.length, *shape[1:]), \n",
    "                                    fill_value=l_datachunk[0].fill, \n",
    "                                    dtype=l_datachunk[0].dtype)\n",
    "            for datachunk in l_datachunk:\n",
    "                full_sequence[datachunk.slice] = datachunk.data\n",
    "            \n",
    "            return full_sequence\n",
    "                \n",
    "    def __iter__(self):             \n",
    "        groups = {\"sync\":[],\"stim\":[],\"data\":[],\"cell\":[]}\n",
    "        for key, dChunk_l in self._data_dict.items():\n",
    "            groups[dChunk_l[0].group].append((dChunk_l[0].idx, key))\n",
    "\n",
    "        self._iter_order = []\n",
    "        for group_name in [\"sync\",\"stim\",\"data\",\"cell\"]:\n",
    "            sorted_ = sorted(groups[group_name], key=lambda e:(e[0],))\n",
    "            self._iter_order.extend([key for _, key in sorted_])\n",
    "        self._n = 0\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._n < len(self._iter_order):\n",
    "            key = self._iter_order[self._n]\n",
    "            dChunk_l = self._data_dict[key]\n",
    "            self._n += 1\n",
    "            return (key, dChunk_l)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def __delitem__(self, key):\n",
    "        del self._data_dict[key]\n",
    "        \n",
    "    def __str__(self):\n",
    "        res = \"ContiguousRecord:\\n\"\n",
    "        for k,v in self._data_dict.items():\n",
    "            res += k+\" : \"+\" \".join([str(dc.shape) for dc in v]) +\"\\n\"\n",
    "        return res\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._data_dict.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RecordMaster(list):\n",
    "    \"\"\"\n",
    "    One Timeserie to rule them all, One Timeserie to find them,\n",
    "    One Timeserie to bring them all and in the darkness bind them\n",
    "    \n",
    "    The RecordMaster class is the top level object managing all\n",
    "    timeseries. It uses a list of ContiguousRecord to represent\n",
    "    possible discontinuted data records.\n",
    "    \n",
    "    The main aim of the RecordMaster is to store the various data\n",
    "    stream of an experiment under a unique time reference, to ease\n",
    "    the processing of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data_list: Sequence[Tuple[DataChunk, DataChunk]], frame_time=1/60, sep_size=1000):\n",
    "        \n",
    "        self.frame_time = frame_time\n",
    "        self.sep_size   = sep_size\n",
    "        self._sequences = []\n",
    "        for ref_timepoints, ref_signals in reference_data_list:\n",
    "            cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "            self._sequences.append(cs)\n",
    "            \n",
    "    def set_datachunk(self, dc:DataChunk, name:str, sequence_idx=0):\n",
    "        \"\"\"Set the given DataChunk dc for the sequence at sequence_idx under name.\"\"\"\n",
    "        self._sequences[sequence_idx][name] = dc\n",
    "        \n",
    "    def append(self, ref_timepoints:DataChunk, ref_signals:DataChunk):\n",
    "        cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "        self._sequences.append(cs)\n",
    "        \n",
    "    def insert(self, idx:int, ref_timepoints:DataChunk, ref_signals:DataChunk):\n",
    "        cs = ContiguousRecord(len(ref_timepoints), ref_signals, ref_timepoints)\n",
    "        self._sequences.insert(idx, cs)\n",
    "        \n",
    "    def __setitem__(self, key, value:DataChunk):\n",
    "        \"\"\"Setting an item directly to the record_master place it in the first sequence\"\"\"\n",
    "        if isinstance(key, str):\n",
    "            self._sequences[0][key] = value\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, (int, np.integer)):\n",
    "            return self._sequences[key]\n",
    "        elif isinstance(key, str):\n",
    "            #We want all the data of that name\n",
    "            res = []\n",
    "            for seq in self._sequences:\n",
    "                res.append(seq[key])\n",
    "            return res\n",
    "\n",
    "        raise TypeError(\"Indexing not understood\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self._n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._n < len(self):\n",
    "            res = self._sequences[self._n]\n",
    "            self._n += 1\n",
    "            return res\n",
    "        else:\n",
    "            raise StopIteration \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._sequences)\n",
    "        \n",
    "    def plot(self):\n",
    "        colors = {\"sync\":\"cornflowerblue\", \"stim\":\"orange\", \"data\":\"yellowgreen\", \"cell\":\"plum\"}\n",
    "        cursor = 0\n",
    "        y_pos_dict = {}\n",
    "        y_count    = 0\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        for seq in self._sequences:\n",
    "            for y, (name, dChunk_l) in enumerate(seq):\n",
    "                for dChunk in dChunk_l:\n",
    "                    pos = dChunk.idx + cursor \n",
    "                    ax.barh(name, len(dChunk), left=pos, height=0.8, color=colors[dChunk.group], label=dChunk.group)\n",
    "                    x = pos + len(dChunk)/2\n",
    "                    text = \"{0} -> {1} \".format(self.to_time_str(dChunk.idx), self.to_time_str(dChunk.idx+len(dChunk)))\n",
    "                    if name not in y_pos_dict.keys():\n",
    "                        y_pos_dict[name] = y_count\n",
    "                        y_count+=1\n",
    "                    y_pos = y_pos_dict[name]\n",
    "                    ax.text(x, y_pos, text, ha='center', va='center')\n",
    "            cursor += len(seq) + self.sep_size\n",
    "            \n",
    "        legend_elements = [Patch(facecolor=colors[\"sync\"],label='Synchro'),\n",
    "                           Patch(facecolor=colors[\"data\"],label='Data'),\n",
    "                           Patch(facecolor=colors[\"stim\"],label='Stimulus'),\n",
    "                           Patch(facecolor=colors[\"cell\"],label='Cell'),]\n",
    "        ax.legend(handles=legend_elements, ncol=5, bbox_to_anchor=(0, 1), loc='lower left', fontsize='small')\n",
    "        \n",
    "        ax.set_xlim(-100,cursor)\n",
    "            \n",
    "    def to_s(self, n_frame):\n",
    "        return round(self.frame_time*n_frame,2)\n",
    "    \n",
    "    def to_time_str(self, n_frame):\n",
    "        s = int(self.to_s(n_frame))\n",
    "        m, s = s//60, str(s%60)\n",
    "        h, m = str(m//60), str(m%60)\n",
    "        return \"{0}:{1}:{2}\".format('0'*(2-len(h))+h, '0'*(2-len(m))+m, '0'*(2-len(s))+s)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"[\"+\",\\n\".join([repr(seq) for seq in self._sequences])+\"]\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"[\"+\", \".join([repr(seq) for seq in self._sequences])+\"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
